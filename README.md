# AAI5003 : NLP must read papers

##  Word Embedding & Recurrent Neural Networks (RNN)
- 2017. Supervised Learning of Universal Sentence Representations from
Natural Language Inference Data [[paper]](https://arxiv.org/pdf/1705.02364.pdf)
- 2016. Man is to Computer Programmer as Woman is to Homemaker?
Debiasing Word Embeddings [[paper]](https://arxiv.org/pdf/1607.06520.pdf)
- 2014. GloVe: Global Vectors for Word Representation [[paper]](https://nlp.stanford.edu/pubs/glove.pdf)
- 2013. Distributed Representations of Words and Phrases
and their Compositionality [[paper]](https://arxiv.org/pdf/1310.4546.pdf)
- 2013. Efficient Estimation of Word Representations in
Vector Space [[paper]](https://arxiv.org/pdf/1301.3781.pdf)

## Multi-task Learning & Sequence-to-Sequence Model
- 2017. Adversarial Multi-task Learning for Text Classification [[paper]](https://arxiv.org/pdf/1704.05742.pdf)
- 2014. Sequence to Sequence Learning
with Neural Networks [[paper]](https://arxiv.org/pdf/1409.3215.pdf)
- 2014. Neural Machine Translation
by Jointly Learning to Align and Translate [[paper]](https://arxiv.org/pdf/1409.0473.pdf)

## Machine Reading Comprehension
- 2017. Reading Wikipedia to Answer Open-Domain Questions [[paper]](https://arxiv.org/pdf/1704.00051.pdf)
- 2016. Bi-Directional Attention Flow for Machine Comprehension [[paper]](https://arxiv.org/pdf/1611.01603.pdf)
- 2016. Text Understanding with the Attention Sum Reader Network [[paper]](https://arxiv.org/pdf/1603.01547v1.pdf)

## Transformer & Attention / Self-attention
- 2019. Attention is not not Explanation [[paper]](https://arxiv.org/pdf/1908.04626.pdf)
- 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach [[paper]](https://arxiv.org/pdf/1907.11692.pdf)
- 2019. XLNet: Generalized Autoregressive Pretraining
for Language Understanding [[paper]](https://arxiv.org/pdf/1906.08237.pdf)
- 2019. Attention is not Explanation [[paper]](https://arxiv.org/pdf/1902.10186.pdf)
- 2018. Deriving Machine Attention from Human Rationales [[paper]](https://arxiv.org/pdf/1808.09367.pdf)
- 2017. Simple and Effective Multi-Paragraph Reading Comprehension [[paper]](https://arxiv.org/pdf/1710.10723.pdf)
- 2017. Attention is All You Need [[paper]](https://arxiv.org/pdf/1706.03762.pdf)

## Pre-trained Language Models
- 2019. What Does BERT Look At?
An Analysis of BERTâ€™s Attention [[paper]](https://arxiv.org/pdf/1906.04341.pdf)
- 2018. BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding [[paper]](https://arxiv.org/pdf/1810.04805.pdf)
- 2018. Improving Language Understanding
by Generative Pre-Training [[paper]](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- 2018. Deep contextualized word representations [[paper]](https://arxiv.org/pdf/1802.05365.pdf)

## Conversational AI : Response Selection / Dialogue System
- 2019. SUMBT: Slot-Utterance Matching
for Universal and Scalable Belief Tracking [[paper]](https://arxiv.org/pdf/1907.07421.pdf)
- 2019. Poly-Encoders: Architectures and Pre-training
Strategies for Fast and Accurate Multi-sentence Scoring [[paper]](https://arxiv.org/pdf/1905.01969.pdf)
- 2018. Dialogue Natural Language Inference [[paper]](https://arxiv.org/pdf/1811.00671.pdf)
- 2018. MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for
Task-Oriented Dialogue Modelling [[paper]](https://arxiv.org/pdf/1810.00278.pdf)
- 2018. Training Millions of Personalized Dialogue Agents [[paper]](https://arxiv.org/pdf/1809.01984.pdf)
- 2018. CoQA: A Conversational Question Answering Challenge [[paper]](https://arxiv.org/pdf/1808.07042.pdf)
- 2018. Personalizing Dialogue Agents: I have a dog, do you have pets too? [[paper]](https://arxiv.org/pdf/1801.07243.pdf)
- 2016. Neural Belief Tracker: Data-Driven Dialogue State Tracking [[paper]](https://arxiv.org/pdf/1606.03777.pdf)

## Knowledge Base / Distillation / Processing
- 2019. Knowledge Enhanced Contextual Word Representations [[paper]](https://arxiv.org/pdf/1909.04164.pdf)
- 2019. ERNIE: Enhanced Language Representation with Informative Entities [[paper]](https://arxiv.org/pdf/1905.07129.pdf)
- 2019. ERNIE: Enhanced Representation through Knowledge Integration [[paper]](https://arxiv.org/pdf/1904.09223.pdf)
- 2019. Multi-Task Deep Neural Networks for Natural Language Understanding [[paper]](https://arxiv.org/pdf/1901.11504.pdf)
- 2018. Born-Again Neural Networks [[paper]](https://arxiv.org/pdf/1805.04770.pdf)
- 2017. Zero-Shot Relation Extraction via Reading Comprehension [[paper]](https://arxiv.org/pdf/1706.04115.pdf)
- 2017. A Knowledge-Grounded Neural Conversation Model [[paper]](https://arxiv.org/pdf/1702.01932.pdf)
- 2015. End-To-End Memory Networks [[paper]](https://arxiv.org/pdf/1503.08895.pdf)
- 2014. Memory Networks [[paper]](https://arxiv.org/pdf/1410.3916.pdf)
- 2013. Translating Embeddings for Modeling
Multi-relational Data [[paper]](https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf)

## Graph Neural Networks
