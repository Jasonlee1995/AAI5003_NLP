# AAI5003 : NLP must read papers
### All the papers are sorted in descending order by publication/submission date

## 1. Benchmark
- GLUE [[paper]](https://arxiv.org/pdf/1804.07461.pdf)

## 2. Dataset
* ### 2.1. Single Sequence Tasks
 * #### 2.1.a. Acceptability Judgement
   - CoLA (in GLUE) [[paper]](https://arxiv.org/pdf/1805.12471.pdf)
 * #### 2.1.b. Sentiment Classification
   - SST-2 (in GLUE) [[paper]](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)
   - IMDB [[paper]](https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf)

* ### 2.2. Sequence Pair Tasks
 * #### 2.2.a. Paraphrase Identification
   - QQP (in GLUE) [[link]](https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs)
   - MRPC (in GLUE) [[paper]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/I05-50025B15D.pdf)
 * #### 2.2.b. Similarity Prediction
   - STS-B (in GLUE) [[paper]](https://arxiv.org/pdf/1708.00055.pdf)
 * #### 2.2.c. Natural Language Inference (NLI)
   - XNLI [[paper]](https://arxiv.org/pdf/1809.05053.pdf)
   - MNLI (in GLUE) [[paper]](https://arxiv.org/pdf/1704.05426.pdf)
   - QNLI (in GLUE) [[paper]](https://arxiv.org/pdf/1606.05250.pdf)
   - SNLI [[paper]](https://arxiv.org/pdf/1508.05326.pdf)
   - RTE (in GLUE) [[link]](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment)
   - WNLI (in GLUE) [[link]](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html)

## 3. Field
* ## 3.1. Word Embedding / Recurrent Neural Networks (RNN)
 - Supervised Learning of Universal Sentence Representations from
Natural Language Inference Data [[paper]](https://arxiv.org/pdf/1705.02364.pdf)
 - Man is to Computer Programmer as Woman is to Homemaker?
Debiasing Word Embeddings [[paper]](https://arxiv.org/pdf/1607.06520.pdf)
 - GloVe: Global Vectors for Word Representation [[paper]](https://nlp.stanford.edu/pubs/glove.pdf)
 - Distributed Representations of Words and Phrases
and their Compositionality [[paper]](https://arxiv.org/pdf/1310.4546.pdf)
 - Efficient Estimation of Word Representations in
Vector Space [[paper]](https://arxiv.org/pdf/1301.3781.pdf)

* ## 3.2. Multi-task Learning / Sequence-to-Sequence Model
 - Adversarial Multi-task Learning for Text Classification [[paper]](https://arxiv.org/pdf/1704.05742.pdf)
 - Sequence to Sequence Learning
with Neural Networks [[paper]](https://arxiv.org/pdf/1409.3215.pdf)
 - Neural Machine Translation
by Jointly Learning to Align and Translate [[paper]](https://arxiv.org/pdf/1409.0473.pdf)

* ## 3.3. Machine Reading Comprehension
 - Reading Wikipedia to Answer Open-Domain Questions [[paper]](https://arxiv.org/pdf/1704.00051.pdf)
 - Bi-Directional Attention Flow for Machine Comprehension [[paper]](https://arxiv.org/pdf/1611.01603.pdf)
 - Text Understanding with the Attention Sum Reader Network [[paper]](https://arxiv.org/pdf/1603.01547v1.pdf)

* ## 3.4. Transformer / Attention & Self-attention
 - Attention is not not Explanation [[paper]](https://arxiv.org/pdf/1908.04626.pdf)
 - RoBERTa: A Robustly Optimized BERT Pretraining Approach [[paper]](https://arxiv.org/pdf/1907.11692.pdf)
 - XLNet: Generalized Autoregressive Pretraining
for Language Understanding [[paper]](https://arxiv.org/pdf/1906.08237.pdf)
 - Attention is not Explanation [[paper]](https://arxiv.org/pdf/1902.10186.pdf)
 - Deriving Machine Attention from Human Rationales [[paper]](https://arxiv.org/pdf/1808.09367.pdf)
 - Simple and Effective Multi-Paragraph Reading Comprehension [[paper]](https://arxiv.org/pdf/1710.10723.pdf)
 - Attention is All You Need [[paper]](https://arxiv.org/pdf/1706.03762.pdf)

* ## 3.5. Pre-trained Language Models
 - What Does BERT Look At?
An Analysis of BERTâ€™s Attention [[paper]](https://arxiv.org/pdf/1906.04341.pdf)
 - BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding [[paper]](https://arxiv.org/pdf/1810.04805.pdf)
 - Improving Language Understanding
by Generative Pre-Training [[paper]](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
 - Deep contextualized word representations [[paper]](https://arxiv.org/pdf/1802.05365.pdf)

* ## 3.6. Conversational AI : Response Selection & Dialogue System
 - SUMBT: Slot-Utterance Matching
for Universal and Scalable Belief Tracking [[paper]](https://arxiv.org/pdf/1907.07421.pdf)
 - Poly-Encoders: Architectures and Pre-training
Strategies for Fast and Accurate Multi-sentence Scoring [[paper]](https://arxiv.org/pdf/1905.01969.pdf)
 - Dialogue Natural Language Inference [[paper]](https://arxiv.org/pdf/1811.00671.pdf)
 - MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for
Task-Oriented Dialogue Modelling [[paper]](https://arxiv.org/pdf/1810.00278.pdf)
 - Training Millions of Personalized Dialogue Agents [[paper]](https://arxiv.org/pdf/1809.01984.pdf)
 - CoQA: A Conversational Question Answering Challenge [[paper]](https://arxiv.org/pdf/1808.07042.pdf)
 - Personalizing Dialogue Agents: I have a dog, do you have pets too? [[paper]](https://arxiv.org/pdf/1801.07243.pdf)
 - Neural Belief Tracker: Data-Driven Dialogue State Tracking [[paper]](https://arxiv.org/pdf/1606.03777.pdf)

* ## 3.7. Knowledge Base & Distillation & Processing
 - Knowledge Enhanced Contextual Word Representations [[paper]](https://arxiv.org/pdf/1909.04164.pdf)
 - ERNIE: Enhanced Language Representation with Informative Entities [[paper]](https://arxiv.org/pdf/1905.07129.pdf)
 - ERNIE: Enhanced Representation through Knowledge Integration [[paper]](https://arxiv.org/pdf/1904.09223.pdf)
 - Multi-Task Deep Neural Networks for Natural Language Understanding [[paper]](https://arxiv.org/pdf/1901.11504.pdf)
 - Born-Again Neural Networks [[paper]](https://arxiv.org/pdf/1805.04770.pdf)
 - Zero-Shot Relation Extraction via Reading Comprehension [[paper]](https://arxiv.org/pdf/1706.04115.pdf)
 - A Knowledge-Grounded Neural Conversation Model [[paper]](https://arxiv.org/pdf/1702.01932.pdf)
 - End-To-End Memory Networks [[paper]](https://arxiv.org/pdf/1503.08895.pdf)
 - Memory Networks [[paper]](https://arxiv.org/pdf/1410.3916.pdf)
 - Translating Embeddings for Modeling
Multi-relational Data [[paper]](https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf)

* ## 3.8. Graph Neural Networks
